{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "ac232977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.calibration import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "77dd3ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4004 entries, 0 to 4003\n",
      "Data columns (total 95 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   loc_rowid        4004 non-null   int64  \n",
      " 1   pl_name          4004 non-null   object \n",
      " 2   hostname         4004 non-null   object \n",
      " 3   default_flag     4004 non-null   int64  \n",
      " 4   disposition      4004 non-null   object \n",
      " 5   disp_refname     4004 non-null   object \n",
      " 6   sy_snum          4004 non-null   int64  \n",
      " 7   sy_pnum          4004 non-null   int64  \n",
      " 8   discoverymethod  4004 non-null   object \n",
      " 9   disc_year        4004 non-null   int64  \n",
      " 10  disc_facility    4004 non-null   object \n",
      " 11  soltype          4004 non-null   object \n",
      " 12  pl_controv_flag  4004 non-null   int64  \n",
      " 13  pl_refname       4004 non-null   object \n",
      " 14  pl_orbper        3960 non-null   float64\n",
      " 15  pl_orbpererr1    3071 non-null   float64\n",
      " 16  pl_orbpererr2    3071 non-null   float64\n",
      " 17  pl_orbperlim     3960 non-null   float64\n",
      " 18  pl_orbsmax       820 non-null    float64\n",
      " 19  pl_orbsmaxerr1   813 non-null    float64\n",
      " 20  pl_orbsmaxerr2   813 non-null    float64\n",
      " 21  pl_orbsmaxlim    820 non-null    float64\n",
      " 22  pl_rade          3173 non-null   float64\n",
      " 23  pl_radeerr1      2887 non-null   float64\n",
      " 24  pl_radeerr2      2887 non-null   float64\n",
      " 25  pl_radelim       3173 non-null   float64\n",
      " 26  pl_radj          3173 non-null   float64\n",
      " 27  pl_radjerr1      2887 non-null   float64\n",
      " 28  pl_radjerr2      2887 non-null   float64\n",
      " 29  pl_radjlim       3173 non-null   float64\n",
      " 30  pl_bmasse        435 non-null    float64\n",
      " 31  pl_bmasseerr1    393 non-null    float64\n",
      " 32  pl_bmasseerr2    393 non-null    float64\n",
      " 33  pl_bmasselim     435 non-null    float64\n",
      " 34  pl_bmassj        435 non-null    float64\n",
      " 35  pl_bmassjerr1    393 non-null    float64\n",
      " 36  pl_bmassjerr2    393 non-null    float64\n",
      " 37  pl_bmassjlim     435 non-null    float64\n",
      " 38  pl_bmassprov     435 non-null    object \n",
      " 39  pl_orbeccen      429 non-null    float64\n",
      " 40  pl_orbeccenerr1  226 non-null    float64\n",
      " 41  pl_orbeccenerr2  226 non-null    float64\n",
      " 42  pl_orbeccenlim   429 non-null    float64\n",
      " 43  pl_insol         630 non-null    float64\n",
      " 44  pl_insolerr1     447 non-null    float64\n",
      " 45  pl_insolerr2     447 non-null    float64\n",
      " 46  pl_insollim      630 non-null    float64\n",
      " 47  pl_eqt           854 non-null    float64\n",
      " 48  pl_eqterr1       685 non-null    float64\n",
      " 49  pl_eqterr2       685 non-null    float64\n",
      " 50  pl_eqtlim        854 non-null    float64\n",
      " 51  ttv_flag         4004 non-null   int64  \n",
      " 52  st_refname       3988 non-null   object \n",
      " 53  st_spectype      437 non-null    object \n",
      " 54  st_teff          2892 non-null   float64\n",
      " 55  st_tefferr1      2584 non-null   float64\n",
      " 56  st_tefferr2      2578 non-null   float64\n",
      " 57  st_tefflim       2892 non-null   float64\n",
      " 58  st_rad           3874 non-null   float64\n",
      " 59  st_raderr1       3242 non-null   float64\n",
      " 60  st_raderr2       3236 non-null   float64\n",
      " 61  st_radlim        3874 non-null   float64\n",
      " 62  st_mass          2102 non-null   float64\n",
      " 63  st_masserr1      1946 non-null   float64\n",
      " 64  st_masserr2      1940 non-null   float64\n",
      " 65  st_masslim       2102 non-null   float64\n",
      " 66  st_met           1698 non-null   float64\n",
      " 67  st_meterr1       1667 non-null   float64\n",
      " 68  st_meterr2       1667 non-null   float64\n",
      " 69  st_metlim        1698 non-null   float64\n",
      " 70  st_metratio      1695 non-null   object \n",
      " 71  st_logg          2356 non-null   float64\n",
      " 72  st_loggerr1      2143 non-null   float64\n",
      " 73  st_loggerr2      2143 non-null   float64\n",
      " 74  st_logglim       2356 non-null   float64\n",
      " 75  sy_refname       4004 non-null   object \n",
      " 76  rastr            4004 non-null   object \n",
      " 77  ra               4004 non-null   float64\n",
      " 78  decstr           4004 non-null   object \n",
      " 79  dec              4004 non-null   float64\n",
      " 80  sy_dist          3879 non-null   float64\n",
      " 81  sy_disterr1      3748 non-null   float64\n",
      " 82  sy_disterr2      3748 non-null   float64\n",
      " 83  sy_vmag          3962 non-null   float64\n",
      " 84  sy_vmagerr1      3962 non-null   float64\n",
      " 85  sy_vmagerr2      3962 non-null   float64\n",
      " 86  sy_kmag          3981 non-null   float64\n",
      " 87  sy_kmagerr1      3973 non-null   float64\n",
      " 88  sy_kmagerr2      3973 non-null   float64\n",
      " 89  sy_gaiamag       3948 non-null   float64\n",
      " 90  sy_gaiamagerr1   3948 non-null   float64\n",
      " 91  sy_gaiamagerr2   3948 non-null   float64\n",
      " 92  rowupdate        4004 non-null   object \n",
      " 93  pl_pubdate       4004 non-null   object \n",
      " 94  releasedate      4004 non-null   object \n",
      "dtypes: float64(70), int64(7), object(18)\n",
      "memory usage: 2.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. Load dataset\n",
    "# -------------------------------\n",
    "df = pd.read_csv('Data\\k2pandc final.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "ac386b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. ]Drop name and ID columns\n",
    "# -------------------------------\n",
    "cols_to_drop = [\n",
    "    # IDs, references, dates\n",
    "    'loc_rowid','pl_name','hostname','disp_refname','pl_refname','st_refname',\n",
    "    'sy_refname','rowupdate','pl_pubdate','releasedate',\n",
    "    \n",
    "    # Orbital/planet mass errors and flags (keep main numeric columns)\n",
    "    'pl_orbsmaxerr1','pl_orbsmaxerr2','pl_orbsmaxlim',\n",
    "    'pl_bmasseerr1','pl_bmasseerr2','pl_bmasselim',\n",
    "    'pl_bmassj','pl_bmassjerr1','pl_bmassjerr2','pl_bmassjlim','pl_bmassprov',\n",
    "    'pl_orbeccen','pl_orbeccenerr1','pl_orbeccenerr2','pl_orbeccenlim',\n",
    "    \n",
    "    # Flux and temperature errors/limits (keep main numeric columns)\n",
    "    'pl_insol','pl_insolerr1','pl_insolerr2','pl_insollim',\n",
    "    'pl_eqterr1','pl_eqterr2','pl_eqtlim',\n",
    "    \n",
    "    # Stellar categorical / low coverage\n",
    "    'st_spectype',\n",
    "    \n",
    "    # Stellar mass/metallicity errors & limits (keep main numeric columns)\n",
    "    'st_masserr1','st_masserr2','st_masslim',\n",
    "    'st_met','st_meterr1','st_meterr2','st_metlim','st_metratio',\n",
    "    \n",
    "    # Planet radius errors & limits\n",
    "    'pl_radjerr1','pl_radjerr2','pl_radjlim',\n",
    "    \n",
    "    # Orbital period errors\n",
    "    'pl_orbpererr1','pl_orbpererr2','pl_orbperlim',\n",
    "    \n",
    "    # Stellar teff & radius errors/limits\n",
    "    'st_tefferr1','st_tefferr2','st_tefflim',\n",
    "    'st_raderr1','st_raderr2','st_radlim',\n",
    "    \n",
    "    # Photometric errors\n",
    "    'sy_vmagerr1','sy_vmagerr2','sy_kmagerr1','sy_kmagerr2',\n",
    "    'sy_gaiamagerr1','sy_gaiamagerr2'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "9786f101",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2.1 Handle missing values (improved)\n",
    "# -------------------------------\n",
    "missing_pct = df.isnull().mean() * 100\n",
    "\n",
    "# Drop columns with >50% missing values\n",
    "cols_to_drop = missing_pct[missing_pct > 50].index\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Update num_cols and cat_cols after dropping columns\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "cat_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "\n",
    "# Fill numerical columns with median\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "# Fill categorical columns with mode\n",
    "for col in cat_cols:\n",
    "    mode = df[col].mode()\n",
    "    df[col] = df[col].fillna(mode[0] if not mode.empty else 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "b61a7aad",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4004 entries, 0 to 4003\n",
      "Data columns (total 33 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   default_flag     4004 non-null   int64  \n",
      " 1   disposition      4004 non-null   object \n",
      " 2   sy_snum          4004 non-null   int64  \n",
      " 3   sy_pnum          4004 non-null   int64  \n",
      " 4   discoverymethod  4004 non-null   object \n",
      " 5   disc_year        4004 non-null   int64  \n",
      " 6   disc_facility    4004 non-null   object \n",
      " 7   soltype          4004 non-null   object \n",
      " 8   pl_controv_flag  4004 non-null   int64  \n",
      " 9   pl_orbper        4004 non-null   float64\n",
      " 10  pl_rade          4004 non-null   float64\n",
      " 11  pl_radeerr1      4004 non-null   float64\n",
      " 12  pl_radeerr2      4004 non-null   float64\n",
      " 13  pl_radelim       4004 non-null   float64\n",
      " 14  pl_radj          4004 non-null   float64\n",
      " 15  ttv_flag         4004 non-null   int64  \n",
      " 16  st_teff          4004 non-null   float64\n",
      " 17  st_rad           4004 non-null   float64\n",
      " 18  st_mass          4004 non-null   float64\n",
      " 19  st_logg          4004 non-null   float64\n",
      " 20  st_loggerr1      4004 non-null   float64\n",
      " 21  st_loggerr2      4004 non-null   float64\n",
      " 22  st_logglim       4004 non-null   float64\n",
      " 23  rastr            4004 non-null   object \n",
      " 24  ra               4004 non-null   float64\n",
      " 25  decstr           4004 non-null   object \n",
      " 26  dec              4004 non-null   float64\n",
      " 27  sy_dist          4004 non-null   float64\n",
      " 28  sy_disterr1      4004 non-null   float64\n",
      " 29  sy_disterr2      4004 non-null   float64\n",
      " 30  sy_vmag          4004 non-null   float64\n",
      " 31  sy_kmag          4004 non-null   float64\n",
      " 32  sy_gaiamag       4004 non-null   float64\n",
      "dtypes: float64(21), int64(6), object(6)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "75259bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Scale numerical features\n",
    "# -------------------------------\n",
    "df_scaled = df.copy()\n",
    "num_cols = df_scaled.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = StandardScaler()\n",
    "df_scaled[num_cols] = scaler.fit_transform(df_scaled[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "8f607fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. Encode categorical features\n",
    "# -------------------------------\n",
    "##########\n",
    "df_encoded = df_scaled.copy()\n",
    "cat_cols = df_encoded.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "\n",
    "le_dict = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col].fillna('NaN_Label'))\n",
    "    le_dict[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "e3d39068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5. Feature selection (correlation + importance)\n",
    "# -------------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = df_encoded.drop(columns=['disposition'])\n",
    "y = df_encoded['disposition']\n",
    "\n",
    "X = pd.get_dummies(X)  # one-hot if needed\n",
    "X_train, _, y_train, _ = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Correlation\n",
    "corr_with_target = df_encoded.corr(numeric_only=True)['disposition'].drop('disposition').abs()\n",
    "top_corr = corr_with_target.sort_values(ascending=False).head(7).index.tolist()\n",
    "\n",
    "# Random Forest importance\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "top_importance = importances.head(7).index.tolist()\n",
    "\n",
    "# Combine\n",
    "selected_features = list(set(top_corr + top_importance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "8acda4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 6. Build preprocessed dataframe\n",
    "# -------------------------------\n",
    "X = df_encoded.drop(columns=['disposition'])\n",
    "valuable_features_df = X[selected_features]\n",
    "\n",
    "# Add target\n",
    "preprocessed_df = valuable_features_df.copy()\n",
    "preprocessed_df['disposition'] = df_encoded['disposition']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "c1750bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4004 entries, 0 to 4003\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sy_dist       4004 non-null   float64\n",
      " 1   sy_vmag       4004 non-null   float64\n",
      " 2   sy_pnum       4004 non-null   float64\n",
      " 3   dec           4004 non-null   float64\n",
      " 4   soltype       4004 non-null   int32  \n",
      " 5   sy_kmag       4004 non-null   float64\n",
      " 6   sy_disterr2   4004 non-null   float64\n",
      " 7   default_flag  4004 non-null   float64\n",
      " 8   pl_radeerr2   4004 non-null   float64\n",
      " 9   disc_year     4004 non-null   float64\n",
      " 10  sy_gaiamag    4004 non-null   float64\n",
      " 11  disposition   4004 non-null   int32  \n",
      "dtypes: float64(10), int32(2)\n",
      "memory usage: 344.2 KB\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "1983b5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disposition\n",
       "1    2315\n",
       "0    1374\n",
       "2     293\n",
       "3      22\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df['disposition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "9b50ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = preprocessed_df[preprocessed_df['disposition'] != 3].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "b6228e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disposition\n",
       "1    2315\n",
       "0    1374\n",
       "2     293\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df['disposition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "121ee947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = preprocessed_df.drop(columns=['disposition'])\n",
    "y = preprocessed_df['disposition']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "43a40552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1 2]\n",
      "Class Weights: tensor([0.9663, 0.5733, 4.5242], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Ensure y_train is numpy\n",
    "if isinstance(y_train, torch.Tensor):\n",
    "    y_train_np = y_train.cpu().numpy()\n",
    "else:\n",
    "    y_train_np = np.array(y_train)\n",
    "\n",
    "# Get unique classes\n",
    "classes = np.unique(y_train_np)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train_np\n",
    ")\n",
    "\n",
    "# Convert to tensor for PyTorch\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "\n",
    "# ---- Focal Loss with class weights ----\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # can be class weights tensor\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)  # probability of true class\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "# ---- Initialize with class weights ----\n",
    "loss_fn = FocalLoss(alpha=class_weights, gamma=2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "410d6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Convert to tensors if not already\n",
    "X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 32  # Adjust based on your data size and memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Compute class weights for imbalance (optional but recommended for recall)\n",
    "class_counts = np.bincount(y_train.numpy())\n",
    "class_weights = 1. / class_counts\n",
    "class_weights = torch.tensor(class_weights / class_weights.sum(), dtype=torch.float32)  # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "508fa912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2986, 11])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "7a451748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_rate=0.3):  # <-- fixed __init__\n",
    "        super(ResidualBlock, self).__init__()  # <-- fixed __init__\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.ln = nn.LayerNorm(out_features)  # Layer norm for better stability\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Projection if dimensions differ\n",
    "        self.proj = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = self.proj(x)\n",
    "        x = F.silu(self.ln(self.fc(x)))  # Swish activation\n",
    "        x = self.dropout(x)\n",
    "        return x + residual  # Skip connection\n",
    "\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, input_size=16, num_classes=3):  # <-- fixed __init__\n",
    "        super(ResidualMLP, self).__init__()  # <-- fixed __init__\n",
    "        self.entry = nn.Linear(input_size, 256)  # Entry layer\n",
    "        \n",
    "        # Residual blocks for depth\n",
    "        self.res_block1 = ResidualBlock(256, 256)\n",
    "        self.res_block2 = ResidualBlock(256, 128)\n",
    "        self.res_block3 = ResidualBlock(128, 128)\n",
    "        self.res_block4 = ResidualBlock(128, 64)\n",
    "        \n",
    "        self.dropout_final = nn.Dropout(0.2)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.silu(self.entry(x))\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.res_block4(x)\n",
    "        \n",
    "        x = self.dropout_final(x)\n",
    "        x = self.fc_out(x)  # Logits output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "133dd06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(y_train)  # unique class labels\n",
    "print(\"Classes:\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "6903ae6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1 2]\n",
      "Class Weights: tensor([0.9663, 0.5733, 4.5242], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Ensure y_train is numpy\n",
    "if isinstance(y_train, torch.Tensor):\n",
    "    y_train_np = y_train.cpu().numpy()\n",
    "else:\n",
    "    y_train_np = np.array(y_train)\n",
    "\n",
    "# Get unique classes\n",
    "classes = np.unique(y_train_np)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train_np\n",
    ")\n",
    "\n",
    "# Convert to tensor for PyTorch\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "\n",
    "# ---- Focal Loss with class weights ----\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # can be class weights tensor\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)  # probability of true class\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "# ---- Initialize with class weights ----\n",
    "loss_fn = FocalLoss(alpha=class_weights, gamma=2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "87958dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Number of GPUs: 1\n",
      "Current Device Index: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current Device Index:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "e5b06266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sy_dist</th>\n",
       "      <th>sy_vmag</th>\n",
       "      <th>sy_pnum</th>\n",
       "      <th>dec</th>\n",
       "      <th>soltype</th>\n",
       "      <th>sy_kmag</th>\n",
       "      <th>sy_disterr2</th>\n",
       "      <th>default_flag</th>\n",
       "      <th>pl_radeerr2</th>\n",
       "      <th>disc_year</th>\n",
       "      <th>sy_gaiamag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.387765</td>\n",
       "      <td>-1.229216</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>1.274274</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.953914</td>\n",
       "      <td>0.188935</td>\n",
       "      <td>-0.906452</td>\n",
       "      <td>0.152988</td>\n",
       "      <td>-0.761754</td>\n",
       "      <td>-1.142112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.387765</td>\n",
       "      <td>-1.229216</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>1.274274</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.953914</td>\n",
       "      <td>0.188935</td>\n",
       "      <td>-0.906452</td>\n",
       "      <td>0.157520</td>\n",
       "      <td>-0.761754</td>\n",
       "      <td>-1.142112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.387765</td>\n",
       "      <td>-1.229216</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>1.274274</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.953914</td>\n",
       "      <td>0.188935</td>\n",
       "      <td>1.103202</td>\n",
       "      <td>0.157353</td>\n",
       "      <td>-0.761754</td>\n",
       "      <td>-1.142112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.541380</td>\n",
       "      <td>-0.764653</td>\n",
       "      <td>-0.815954</td>\n",
       "      <td>-0.498382</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.059170</td>\n",
       "      <td>0.197710</td>\n",
       "      <td>1.103202</td>\n",
       "      <td>0.159915</td>\n",
       "      <td>0.222931</td>\n",
       "      <td>-0.831989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.541380</td>\n",
       "      <td>-0.764653</td>\n",
       "      <td>-0.815954</td>\n",
       "      <td>-0.498382</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.059170</td>\n",
       "      <td>0.197710</td>\n",
       "      <td>-0.906452</td>\n",
       "      <td>0.156657</td>\n",
       "      <td>0.222931</td>\n",
       "      <td>-0.831989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>-0.457913</td>\n",
       "      <td>-1.297472</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>0.348257</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.405518</td>\n",
       "      <td>0.189295</td>\n",
       "      <td>-0.906452</td>\n",
       "      <td>-0.719784</td>\n",
       "      <td>-0.761754</td>\n",
       "      <td>-1.281380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>-0.639681</td>\n",
       "      <td>-1.535574</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>-0.489871</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.199202</td>\n",
       "      <td>0.201801</td>\n",
       "      <td>1.103202</td>\n",
       "      <td>0.160793</td>\n",
       "      <td>0.222931</td>\n",
       "      <td>-1.702106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>-0.639681</td>\n",
       "      <td>-1.535574</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>-0.489871</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.199202</td>\n",
       "      <td>0.201801</td>\n",
       "      <td>-0.906452</td>\n",
       "      <td>0.160474</td>\n",
       "      <td>0.222931</td>\n",
       "      <td>-1.702106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>-0.639681</td>\n",
       "      <td>-1.535574</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>-0.489871</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.199202</td>\n",
       "      <td>0.201801</td>\n",
       "      <td>-0.906452</td>\n",
       "      <td>0.160793</td>\n",
       "      <td>0.222931</td>\n",
       "      <td>-1.702106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>-0.639681</td>\n",
       "      <td>-1.535574</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>-0.489871</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.199202</td>\n",
       "      <td>0.201801</td>\n",
       "      <td>-0.906452</td>\n",
       "      <td>0.077437</td>\n",
       "      <td>0.222931</td>\n",
       "      <td>-1.702106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3982 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sy_dist   sy_vmag   sy_pnum       dec  soltype   sy_kmag  sy_disterr2  \\\n",
       "0    -0.387765 -1.229216 -0.091546  1.274274        1 -0.953914     0.188935   \n",
       "1    -0.387765 -1.229216 -0.091546  1.274274        1 -0.953914     0.188935   \n",
       "2    -0.387765 -1.229216 -0.091546  1.274274        1 -0.953914     0.188935   \n",
       "3    -0.541380 -0.764653 -0.815954 -0.498382        0 -1.059170     0.197710   \n",
       "4    -0.541380 -0.764653 -0.815954 -0.498382        0 -1.059170     0.197710   \n",
       "...        ...       ...       ...       ...      ...       ...          ...   \n",
       "3977 -0.457913 -1.297472 -0.091546  0.348257        0 -1.405518     0.189295   \n",
       "3978 -0.639681 -1.535574 -0.091546 -0.489871        1 -2.199202     0.201801   \n",
       "3979 -0.639681 -1.535574 -0.091546 -0.489871        1 -2.199202     0.201801   \n",
       "3980 -0.639681 -1.535574 -0.091546 -0.489871        1 -2.199202     0.201801   \n",
       "3981 -0.639681 -1.535574 -0.091546 -0.489871        2 -2.199202     0.201801   \n",
       "\n",
       "      default_flag  pl_radeerr2  disc_year  sy_gaiamag  \n",
       "0        -0.906452     0.152988  -0.761754   -1.142112  \n",
       "1        -0.906452     0.157520  -0.761754   -1.142112  \n",
       "2         1.103202     0.157353  -0.761754   -1.142112  \n",
       "3         1.103202     0.159915   0.222931   -0.831989  \n",
       "4        -0.906452     0.156657   0.222931   -0.831989  \n",
       "...            ...          ...        ...         ...  \n",
       "3977     -0.906452    -0.719784  -0.761754   -1.281380  \n",
       "3978      1.103202     0.160793   0.222931   -1.702106  \n",
       "3979     -0.906452     0.160474   0.222931   -1.702106  \n",
       "3980     -0.906452     0.160793   0.222931   -1.702106  \n",
       "3981     -0.906452     0.077437   0.222931   -1.702106  \n",
       "\n",
       "[3982 rows x 11 columns]"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "42bb2b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model at epoch 1 | Test Acc: 90.86%\n",
      "Epoch 1/500 | Train Loss: 0.6364, Train Acc: 80.14% | Test Loss: 0.5749, Test Acc: 90.86%\n",
      "⏳ No improvement for 1/70 epochs\n",
      "Epoch 2/500 | Train Loss: 0.4757, Train Acc: 85.10% | Test Loss: 0.4156, Test Acc: 90.06%\n",
      "⏳ No improvement for 2/70 epochs\n",
      "Epoch 3/500 | Train Loss: 0.4150, Train Acc: 87.74% | Test Loss: 0.3499, Test Acc: 88.76%\n",
      "✅ Saved new best model at epoch 4 | Test Acc: 91.37%\n",
      "Epoch 4/500 | Train Loss: 0.4065, Train Acc: 87.88% | Test Loss: 0.3775, Test Acc: 91.37%\n",
      "⏳ No improvement for 1/70 epochs\n",
      "Epoch 5/500 | Train Loss: 0.3673, Train Acc: 88.71% | Test Loss: 0.3402, Test Acc: 87.65%\n",
      "✅ Saved new best model at epoch 6 | Test Acc: 94.18%\n",
      "Epoch 6/500 | Train Loss: 0.3878, Train Acc: 87.24% | Test Loss: 0.4008, Test Acc: 94.18%\n",
      "⏳ No improvement for 1/70 epochs\n",
      "Epoch 7/500 | Train Loss: 0.3792, Train Acc: 89.55% | Test Loss: 0.3059, Test Acc: 92.37%\n",
      "⏳ No improvement for 2/70 epochs\n",
      "Epoch 8/500 | Train Loss: 0.3074, Train Acc: 91.02% | Test Loss: 0.3064, Test Acc: 89.56%\n",
      "⏳ No improvement for 3/70 epochs\n",
      "Epoch 9/500 | Train Loss: 0.3193, Train Acc: 90.35% | Test Loss: 0.3348, Test Acc: 87.45%\n",
      "⏳ No improvement for 4/70 epochs\n",
      "Epoch 10/500 | Train Loss: 0.3067, Train Acc: 91.33% | Test Loss: 0.2849, Test Acc: 89.16%\n",
      "⏳ No improvement for 5/70 epochs\n",
      "Epoch 11/500 | Train Loss: 0.3199, Train Acc: 91.19% | Test Loss: 0.2612, Test Acc: 91.16%\n",
      "⏳ No improvement for 6/70 epochs\n",
      "Epoch 12/500 | Train Loss: 0.3433, Train Acc: 90.52% | Test Loss: 0.2584, Test Acc: 92.77%\n",
      "📉 Learning rate reduced from 0.005000 to 0.004000\n",
      "⏳ No improvement for 7/70 epochs\n",
      "Epoch 13/500 | Train Loss: 0.2781, Train Acc: 91.86% | Test Loss: 0.2674, Test Acc: 93.88%\n",
      "⏳ No improvement for 8/70 epochs\n",
      "Epoch 14/500 | Train Loss: 0.2669, Train Acc: 92.60% | Test Loss: 0.2915, Test Acc: 90.96%\n",
      "⏳ No improvement for 9/70 epochs\n",
      "Epoch 15/500 | Train Loss: 0.2407, Train Acc: 93.27% | Test Loss: 0.2858, Test Acc: 89.96%\n",
      "⏳ No improvement for 10/70 epochs\n",
      "Epoch 16/500 | Train Loss: 0.2543, Train Acc: 93.54% | Test Loss: 0.2775, Test Acc: 91.16%\n",
      "⏳ No improvement for 11/70 epochs\n",
      "Epoch 17/500 | Train Loss: 0.2722, Train Acc: 92.26% | Test Loss: 0.2678, Test Acc: 93.07%\n",
      "⏳ No improvement for 12/70 epochs\n",
      "Epoch 18/500 | Train Loss: 0.2450, Train Acc: 93.40% | Test Loss: 0.2653, Test Acc: 90.26%\n",
      "⏳ No improvement for 13/70 epochs\n",
      "Epoch 19/500 | Train Loss: 0.2365, Train Acc: 93.37% | Test Loss: 0.2857, Test Acc: 93.67%\n",
      "📉 Learning rate reduced from 0.004000 to 0.003200\n",
      "⏳ No improvement for 14/70 epochs\n",
      "Epoch 20/500 | Train Loss: 0.2521, Train Acc: 93.47% | Test Loss: 0.2702, Test Acc: 92.17%\n",
      "✅ Saved new best model at epoch 21 | Test Acc: 94.48%\n",
      "Epoch 21/500 | Train Loss: 0.2288, Train Acc: 93.94% | Test Loss: 0.3135, Test Acc: 94.48%\n",
      "⏳ No improvement for 1/70 epochs\n",
      "Epoch 22/500 | Train Loss: 0.2152, Train Acc: 94.57% | Test Loss: 0.2466, Test Acc: 92.77%\n",
      "⏳ No improvement for 2/70 epochs\n",
      "Epoch 23/500 | Train Loss: 0.2232, Train Acc: 94.14% | Test Loss: 0.2662, Test Acc: 92.17%\n",
      "⏳ No improvement for 3/70 epochs\n",
      "Epoch 24/500 | Train Loss: 0.2192, Train Acc: 94.37% | Test Loss: 0.3169, Test Acc: 91.16%\n",
      "⏳ No improvement for 4/70 epochs\n",
      "Epoch 25/500 | Train Loss: 0.2268, Train Acc: 94.37% | Test Loss: 0.3226, Test Acc: 94.48%\n",
      "✅ Saved new best model at epoch 26 | Test Acc: 94.98%\n",
      "Epoch 26/500 | Train Loss: 0.2154, Train Acc: 94.44% | Test Loss: 0.3654, Test Acc: 94.98%\n",
      "⏳ No improvement for 1/70 epochs\n",
      "Epoch 27/500 | Train Loss: 0.2290, Train Acc: 93.77% | Test Loss: 0.2990, Test Acc: 93.67%\n",
      "⏳ No improvement for 2/70 epochs\n",
      "Epoch 28/500 | Train Loss: 0.2180, Train Acc: 94.07% | Test Loss: 0.3031, Test Acc: 91.77%\n",
      "⏳ No improvement for 3/70 epochs\n",
      "Epoch 29/500 | Train Loss: 0.2267, Train Acc: 94.54% | Test Loss: 0.2483, Test Acc: 92.97%\n",
      "⏳ No improvement for 4/70 epochs\n",
      "Epoch 30/500 | Train Loss: 0.2205, Train Acc: 94.07% | Test Loss: 0.3289, Test Acc: 92.27%\n",
      "⏳ No improvement for 5/70 epochs\n",
      "Epoch 31/500 | Train Loss: 0.2140, Train Acc: 94.41% | Test Loss: 0.3362, Test Acc: 93.67%\n",
      "⏳ No improvement for 6/70 epochs\n",
      "Epoch 32/500 | Train Loss: 0.2217, Train Acc: 94.07% | Test Loss: 0.3256, Test Acc: 91.27%\n",
      "📉 Learning rate reduced from 0.003200 to 0.002560\n",
      "⏳ No improvement for 7/70 epochs\n",
      "Epoch 33/500 | Train Loss: 0.2075, Train Acc: 94.27% | Test Loss: 0.3725, Test Acc: 93.17%\n",
      "⏳ No improvement for 8/70 epochs\n",
      "Epoch 34/500 | Train Loss: 0.2144, Train Acc: 94.31% | Test Loss: 0.3172, Test Acc: 93.78%\n",
      "⏳ No improvement for 9/70 epochs\n",
      "Epoch 35/500 | Train Loss: 0.1986, Train Acc: 94.41% | Test Loss: 0.3380, Test Acc: 94.38%\n",
      "⏳ No improvement for 10/70 epochs\n",
      "Epoch 36/500 | Train Loss: 0.1960, Train Acc: 94.88% | Test Loss: 0.2906, Test Acc: 94.28%\n",
      "⏳ No improvement for 11/70 epochs\n",
      "Epoch 37/500 | Train Loss: 0.2028, Train Acc: 95.08% | Test Loss: 0.3194, Test Acc: 92.97%\n",
      "⏳ No improvement for 12/70 epochs\n",
      "Epoch 38/500 | Train Loss: 0.1982, Train Acc: 94.98% | Test Loss: 0.2711, Test Acc: 92.47%\n",
      "⏳ No improvement for 13/70 epochs\n",
      "Epoch 39/500 | Train Loss: 0.1901, Train Acc: 94.98% | Test Loss: 0.2997, Test Acc: 93.78%\n",
      "📉 Learning rate reduced from 0.002560 to 0.002048\n",
      "⏳ No improvement for 14/70 epochs\n",
      "Epoch 40/500 | Train Loss: 0.1901, Train Acc: 95.45% | Test Loss: 0.3375, Test Acc: 92.17%\n",
      "⏳ No improvement for 15/70 epochs\n",
      "Epoch 41/500 | Train Loss: 0.1833, Train Acc: 95.18% | Test Loss: 0.3762, Test Acc: 92.77%\n",
      "⏳ No improvement for 16/70 epochs\n",
      "Epoch 42/500 | Train Loss: 0.1696, Train Acc: 95.75% | Test Loss: 0.3188, Test Acc: 94.68%\n",
      "⏳ No improvement for 17/70 epochs\n",
      "Epoch 43/500 | Train Loss: 0.1746, Train Acc: 95.68% | Test Loss: 0.3513, Test Acc: 93.78%\n",
      "⏳ No improvement for 18/70 epochs\n",
      "Epoch 44/500 | Train Loss: 0.1841, Train Acc: 95.28% | Test Loss: 0.3133, Test Acc: 94.88%\n",
      "⏳ No improvement for 19/70 epochs\n",
      "Epoch 45/500 | Train Loss: 0.1870, Train Acc: 95.28% | Test Loss: 0.3571, Test Acc: 93.67%\n",
      "⏳ No improvement for 20/70 epochs\n",
      "Epoch 46/500 | Train Loss: 0.1879, Train Acc: 95.81% | Test Loss: 0.3385, Test Acc: 94.18%\n",
      "📉 Learning rate reduced from 0.002048 to 0.001638\n",
      "⏳ No improvement for 21/70 epochs\n",
      "Epoch 47/500 | Train Loss: 0.1861, Train Acc: 95.14% | Test Loss: 0.3909, Test Acc: 92.97%\n",
      "⏳ No improvement for 22/70 epochs\n",
      "Epoch 48/500 | Train Loss: 0.1678, Train Acc: 95.51% | Test Loss: 0.3313, Test Acc: 93.57%\n",
      "⏳ No improvement for 23/70 epochs\n",
      "Epoch 49/500 | Train Loss: 0.1546, Train Acc: 96.45% | Test Loss: 0.3660, Test Acc: 93.67%\n",
      "⏳ No improvement for 24/70 epochs\n",
      "Epoch 50/500 | Train Loss: 0.1659, Train Acc: 95.91% | Test Loss: 0.4812, Test Acc: 93.88%\n",
      "⏳ No improvement for 25/70 epochs\n",
      "Epoch 51/500 | Train Loss: 0.1618, Train Acc: 95.98% | Test Loss: 0.3717, Test Acc: 93.78%\n",
      "⏳ No improvement for 26/70 epochs\n",
      "Epoch 52/500 | Train Loss: 0.1441, Train Acc: 96.42% | Test Loss: 0.3934, Test Acc: 94.58%\n",
      "⏳ No improvement for 27/70 epochs\n",
      "Epoch 53/500 | Train Loss: 0.1576, Train Acc: 96.05% | Test Loss: 0.3998, Test Acc: 94.88%\n",
      "📉 Learning rate reduced from 0.001638 to 0.001311\n",
      "⏳ No improvement for 28/70 epochs\n",
      "Epoch 54/500 | Train Loss: 0.1488, Train Acc: 96.28% | Test Loss: 0.3568, Test Acc: 94.18%\n",
      "⏳ No improvement for 29/70 epochs\n",
      "Epoch 55/500 | Train Loss: 0.1460, Train Acc: 96.48% | Test Loss: 0.3767, Test Acc: 94.08%\n",
      "⏳ No improvement for 30/70 epochs\n",
      "Epoch 56/500 | Train Loss: 0.1387, Train Acc: 96.62% | Test Loss: 0.3783, Test Acc: 94.08%\n",
      "⏳ No improvement for 31/70 epochs\n",
      "Epoch 57/500 | Train Loss: 0.1408, Train Acc: 96.58% | Test Loss: 0.4234, Test Acc: 93.98%\n",
      "⏳ No improvement for 32/70 epochs\n",
      "Epoch 58/500 | Train Loss: 0.1398, Train Acc: 96.08% | Test Loss: 0.4830, Test Acc: 93.78%\n",
      "⏳ No improvement for 33/70 epochs\n",
      "Epoch 59/500 | Train Loss: 0.1392, Train Acc: 96.68% | Test Loss: 0.3945, Test Acc: 93.37%\n",
      "⏳ No improvement for 34/70 epochs\n",
      "Epoch 60/500 | Train Loss: 0.1501, Train Acc: 96.18% | Test Loss: 0.4074, Test Acc: 93.67%\n",
      "📉 Learning rate reduced from 0.001311 to 0.001049\n",
      "⏳ No improvement for 35/70 epochs\n",
      "Epoch 61/500 | Train Loss: 0.1457, Train Acc: 96.32% | Test Loss: 0.3553, Test Acc: 92.67%\n",
      "⏳ No improvement for 36/70 epochs\n",
      "Epoch 62/500 | Train Loss: 0.1609, Train Acc: 95.88% | Test Loss: 0.3348, Test Acc: 93.67%\n",
      "⏳ No improvement for 37/70 epochs\n",
      "Epoch 63/500 | Train Loss: 0.1408, Train Acc: 96.28% | Test Loss: 0.3562, Test Acc: 94.28%\n",
      "⏳ No improvement for 38/70 epochs\n",
      "Epoch 64/500 | Train Loss: 0.1359, Train Acc: 96.65% | Test Loss: 0.3744, Test Acc: 94.18%\n",
      "⏳ No improvement for 39/70 epochs\n",
      "Epoch 65/500 | Train Loss: 0.1394, Train Acc: 96.65% | Test Loss: 0.3618, Test Acc: 94.08%\n",
      "⏳ No improvement for 40/70 epochs\n",
      "Epoch 66/500 | Train Loss: 0.1422, Train Acc: 96.65% | Test Loss: 0.3972, Test Acc: 94.28%\n",
      "⏳ No improvement for 41/70 epochs\n",
      "Epoch 67/500 | Train Loss: 0.1296, Train Acc: 96.78% | Test Loss: 0.3976, Test Acc: 94.48%\n",
      "📉 Learning rate reduced from 0.001049 to 0.000839\n",
      "⏳ No improvement for 42/70 epochs\n",
      "Epoch 68/500 | Train Loss: 0.1167, Train Acc: 96.92% | Test Loss: 0.4621, Test Acc: 93.98%\n",
      "⏳ No improvement for 43/70 epochs\n",
      "Epoch 69/500 | Train Loss: 0.1236, Train Acc: 96.75% | Test Loss: 0.3951, Test Acc: 94.28%\n",
      "⏳ No improvement for 44/70 epochs\n",
      "Epoch 70/500 | Train Loss: 0.1197, Train Acc: 97.02% | Test Loss: 0.4230, Test Acc: 94.58%\n",
      "⏳ No improvement for 45/70 epochs\n",
      "Epoch 71/500 | Train Loss: 0.1133, Train Acc: 97.22% | Test Loss: 0.4419, Test Acc: 93.78%\n",
      "⏳ No improvement for 46/70 epochs\n",
      "Epoch 72/500 | Train Loss: 0.1192, Train Acc: 97.19% | Test Loss: 0.4661, Test Acc: 93.88%\n",
      "⏳ No improvement for 47/70 epochs\n",
      "Epoch 73/500 | Train Loss: 0.1363, Train Acc: 96.55% | Test Loss: 0.4157, Test Acc: 94.48%\n",
      "⏳ No improvement for 48/70 epochs\n",
      "Epoch 74/500 | Train Loss: 0.1183, Train Acc: 97.09% | Test Loss: 0.4363, Test Acc: 94.18%\n",
      "📉 Learning rate reduced from 0.000839 to 0.000671\n",
      "⏳ No improvement for 49/70 epochs\n",
      "Epoch 75/500 | Train Loss: 0.1233, Train Acc: 97.25% | Test Loss: 0.4376, Test Acc: 94.08%\n",
      "⏳ No improvement for 50/70 epochs\n",
      "Epoch 76/500 | Train Loss: 0.1129, Train Acc: 97.35% | Test Loss: 0.4650, Test Acc: 94.18%\n",
      "⏳ No improvement for 51/70 epochs\n",
      "Epoch 77/500 | Train Loss: 0.0986, Train Acc: 97.25% | Test Loss: 0.5126, Test Acc: 94.48%\n",
      "⏳ No improvement for 52/70 epochs\n",
      "Epoch 78/500 | Train Loss: 0.1066, Train Acc: 97.39% | Test Loss: 0.4654, Test Acc: 93.98%\n",
      "⏳ No improvement for 53/70 epochs\n",
      "Epoch 79/500 | Train Loss: 0.1183, Train Acc: 97.12% | Test Loss: 0.4370, Test Acc: 93.67%\n",
      "⏳ No improvement for 54/70 epochs\n",
      "Epoch 80/500 | Train Loss: 0.1107, Train Acc: 97.02% | Test Loss: 0.4343, Test Acc: 94.08%\n",
      "⏳ No improvement for 55/70 epochs\n",
      "Epoch 81/500 | Train Loss: 0.0900, Train Acc: 97.42% | Test Loss: 0.4934, Test Acc: 94.98%\n",
      "📉 Learning rate reduced from 0.000671 to 0.000537\n",
      "⏳ No improvement for 56/70 epochs\n",
      "Epoch 82/500 | Train Loss: 0.1070, Train Acc: 97.45% | Test Loss: 0.4934, Test Acc: 93.78%\n",
      "⏳ No improvement for 57/70 epochs\n",
      "Epoch 83/500 | Train Loss: 0.1108, Train Acc: 97.22% | Test Loss: 0.5055, Test Acc: 94.68%\n",
      "⏳ No improvement for 58/70 epochs\n",
      "Epoch 84/500 | Train Loss: 0.0944, Train Acc: 97.59% | Test Loss: 0.4965, Test Acc: 94.68%\n",
      "⏳ No improvement for 59/70 epochs\n",
      "Epoch 85/500 | Train Loss: 0.1106, Train Acc: 97.62% | Test Loss: 0.4085, Test Acc: 94.78%\n",
      "⏳ No improvement for 60/70 epochs\n",
      "Epoch 86/500 | Train Loss: 0.1013, Train Acc: 97.19% | Test Loss: 0.4423, Test Acc: 94.28%\n",
      "⏳ No improvement for 61/70 epochs\n",
      "Epoch 87/500 | Train Loss: 0.1022, Train Acc: 97.35% | Test Loss: 0.4898, Test Acc: 93.98%\n",
      "⏳ No improvement for 62/70 epochs\n",
      "Epoch 88/500 | Train Loss: 0.0883, Train Acc: 97.76% | Test Loss: 0.4727, Test Acc: 94.28%\n",
      "📉 Learning rate reduced from 0.000537 to 0.000429\n",
      "⏳ No improvement for 63/70 epochs\n",
      "Epoch 89/500 | Train Loss: 0.0992, Train Acc: 97.25% | Test Loss: 0.4580, Test Acc: 94.48%\n",
      "⏳ No improvement for 64/70 epochs\n",
      "Epoch 90/500 | Train Loss: 0.0924, Train Acc: 97.82% | Test Loss: 0.4860, Test Acc: 94.28%\n",
      "⏳ No improvement for 65/70 epochs\n",
      "Epoch 91/500 | Train Loss: 0.0862, Train Acc: 97.62% | Test Loss: 0.5020, Test Acc: 94.28%\n",
      "⏳ No improvement for 66/70 epochs\n",
      "Epoch 92/500 | Train Loss: 0.0982, Train Acc: 97.39% | Test Loss: 0.4956, Test Acc: 94.08%\n",
      "⏳ No improvement for 67/70 epochs\n",
      "Epoch 93/500 | Train Loss: 0.0894, Train Acc: 97.89% | Test Loss: 0.5007, Test Acc: 94.78%\n",
      "⏳ No improvement for 68/70 epochs\n",
      "Epoch 94/500 | Train Loss: 0.0988, Train Acc: 97.69% | Test Loss: 0.5456, Test Acc: 93.67%\n",
      "⏳ No improvement for 69/70 epochs\n",
      "Epoch 95/500 | Train Loss: 0.0947, Train Acc: 97.39% | Test Loss: 0.4950, Test Acc: 94.08%\n",
      "📉 Learning rate reduced from 0.000429 to 0.000344\n",
      "⏳ No improvement for 70/70 epochs\n",
      "🚨 Early stopping triggered at epoch 96\n",
      "🎯 Best Test Accuracy: 94.98%\n",
      "\n",
      "🎯 Training finished.\n",
      "Best Test Accuracy: 94.98%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train.shape[1]  # number of features\n",
    "model = ResidualMLP(input_size=input_size, num_classes=3).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.005, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.8, patience=6, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Early stopping setup\n",
    "patience = 70\n",
    "best_accuracy = 0.0\n",
    "epochs_no_improve = 0\n",
    "best_model_path = \"K2_best_residual_mlp_model.pth\"\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    model.eval()\n",
    "    test_loss, correct_test, total_test = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += batch_y.size(0)\n",
    "            correct_test += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "\n",
    "    # --- Scheduler step (manual print) ---\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(test_accuracy)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr < old_lr:\n",
    "        print(f\"📉 Learning rate reduced from {old_lr:.6f} to {new_lr:.6f}\")\n",
    "\n",
    "    # --- Early stopping check ---\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"✅ Saved new best model at epoch {epoch+1} | Test Acc: {best_accuracy:.2f}%\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"⏳ No improvement for {epochs_no_improve}/{patience} epochs\")\n",
    "    \n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"🚨 Early stopping triggered at epoch {epoch+1}\")\n",
    "        print(f\"🎯 Best Test Accuracy: {best_accuracy:.2f}%\")\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        break\n",
    "\n",
    "    # --- Logging ---\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} | \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n",
    "        f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\"\n",
    "    )\n",
    "\n",
    "print(\"\\n🎯 Training finished.\")\n",
    "print(f\"Best Test Accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "4ff6c252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9509    0.9012    0.9254       344\n",
      "           1     0.9914    1.0000    0.9957       579\n",
      "           2     0.6628    0.7808    0.7170        73\n",
      "\n",
      "    accuracy                         0.9498       996\n",
      "   macro avg     0.8684    0.8940    0.8794       996\n",
      "weighted avg     0.9534    0.9498    0.9510       996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- Evaluate on test set and generate classification report ---\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Generate report\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "a0935c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 200 rows from the original dataframe for test set\n",
    "df = pd.read_csv('Data\\k2pandc final.csv')\n",
    "test_sample_df = df.sample(n=200, random_state=42)\n",
    "\n",
    "# Save to CSV\n",
    "test_sample_df.to_csv(\"test_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6491681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
